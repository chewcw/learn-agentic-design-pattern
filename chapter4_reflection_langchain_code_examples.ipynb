{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NBB8hsfc7iJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized: gpt-oss:20b\n",
      "\n",
      "--- Running Reflection Example for Product: 'A mug that keeps coffee hot and can be controlled by a smartphone app.' ---\n",
      "\n",
      "--- Final Refined Product Description ---\n",
      "**Smart Mug – Your Coffee, Perfectly Hot All Day**\n",
      "\n",
      "Never sip lukewarm coffee again. The Smart Mug’s built‑in heating element keeps your brew between **155 °F and 185 °F for up to 6 hours** on a single charge. Its **10‑oz, BPA‑free ceramic** body is leak‑proof, lightweight, and stays hands‑dry—ideal for the office, gym, or road trip.  \n",
      "Control the temperature, set a 30‑minute timer, and receive alerts straight to your smartphone—no guessing, no waste. Feel the comforting warmth of a freshly brewed cup wherever you go.  \n",
      "\n",
      "**Order today** and savor every sip exactly the way you like it.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# --- Configuration ---\n",
    "# Ensure your API key environment variable is set (e.g., OPENAI_API_KEY)\n",
    "try:\n",
    "    # llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    # print(f\"Language model initialized: {llm.model_name}\")\n",
    "    llm = ChatOllama(model=\"gpt-oss:20b\", temperature=0.7)\n",
    "    print(f\"Language model initialized: {llm.model}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing language model: {e}\", file=sys.stderr)\n",
    "    print(\"Please ensure your OPENAI_API_KEY is set correctly.\", file=sys.stderr)\n",
    "    sys.exit(1) # Exit if the LLM cannot be initialized\n",
    "\n",
    "\n",
    "# --- Define Chain Components ---\n",
    "\n",
    "# 1. Initial Generation: Creates the first draft of the product description.\n",
    "# The input to this chain will be a dictionary, so we update the prompt template.\n",
    "generation_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Write a short, simple product description for a new smart coffee mug.\"),\n",
    "        (\"user\", \"{product_details}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 2. Critique: Evaluates the generated description and provides feedback.\n",
    "critique_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Critique the following product description based on clarity, conciseness, and appeal.\n",
    "        Provide specific suggestions for improvement.\"\"\"),\n",
    "        # This will receive 'initial_description' from the previous step.\n",
    "        (\"user\", \"Product Description to Critique:\\n{initial_description}\")\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 3. Refinement: Rewrites the description based on the original details and the critique.\n",
    "refinement_chain = (\n",
    "    ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Based on the original product details and the following critique,\n",
    "        rewrite the product description to be more effective.\n",
    "\n",
    "        Original Product Details: {product_details}\n",
    "        Critique: {critique}\n",
    "\n",
    "        Refined Product Description:\"\"\"),\n",
    "        (\"user\", \"\") # User input is empty as the context is provided in the system message\n",
    "    ])\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "# --- Build the Full Reflection Chain (Refactored) ---\n",
    "# This chain is structured to be more readable and linear.\n",
    "full_reflection_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        initial_description=generation_chain\n",
    "    )\n",
    "    | RunnablePassthrough.assign(\n",
    "        critique=critique_chain\n",
    "    )\n",
    "    | refinement_chain\n",
    ")\n",
    "\n",
    "\n",
    "# --- Run the Chain ---\n",
    "async def run_reflection_example(product_details: str):\n",
    "    \"\"\"Runs the LangChain reflection example with product details.\"\"\"\n",
    "    print(f\"\\n--- Running Reflection Example for Product: '{product_details}' ---\")\n",
    "    try:\n",
    "        # The chain now expects a dictionary as input from the start.\n",
    "        final_refined_description = await full_reflection_chain.ainvoke(\n",
    "            {\"product_details\": product_details}\n",
    "        )\n",
    "        print(\"\\n--- Final Refined Product Description ---\")\n",
    "        print(final_refined_description)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during chain execution: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_product_details = \"A mug that keeps coffee hot and can be controlled by a smartphone app.\"\n",
    "    # asyncio.run(run_reflection_example(test_product_details))\n",
    "    await run_reflection_example(test_product_details)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
